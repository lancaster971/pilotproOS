version: '3.8'

# MILHENA AI Agent with Ollama in Docker
# Ultra-light configuration for GitHub deployment

services:
  # Ollama service with Gemma:2b model
  ollama-milhena:
    image: ollama/ollama:latest
    container_name: pilotpros-ollama-milhena
    ports:
      - "11434:11434"
    volumes:
      - ollama_milhena_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MODELS=/root/.ollama
      - OLLAMA_MAX_LOADED_MODELS=1  # Ultra-light: single model only
      - OLLAMA_NUM_PARALLEL=1       # Single request processing
      - OLLAMA_FLASH_ATTENTION=false # Reduce memory usage
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/health"]
      interval: 60s
      timeout: 15s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 3G    # Hard limit for Gemma:2b
        reservations:
          memory: 2.5G  # Ensure model loads

  # Model initialization service
  ollama-setup:
    image: ollama/ollama:latest
    depends_on:
      ollama-milhena:
        condition: service_healthy
    volumes:
      - ollama_milhena_models:/root/.ollama
    environment:
      - OLLAMA_HOST=ollama-milhena:11434
    entrypoint: >
      sh -c "
        echo 'ðŸ¤– MILHENA - Downloading Gemma:2b model...' &&
        ollama pull gemma:2b &&
        echo 'âœ… Gemma:2b ready for MILHENA!' &&
        echo 'ðŸ§ª Testing model...' &&
        ollama run gemma:2b 'Ciao, sono MILHENA e funziono correttamente!'
      "
    restart: "no"

  # MILHENA AI Agent service
  milhena-agent:
    build:
      context: ../ai-agent
      dockerfile: Dockerfile.milhena
    container_name: pilotpros-milhena-agent
    ports:
      - "3002:3002"
    environment:
      - NODE_ENV=production
      - OLLAMA_URL=http://ollama-milhena:11434
      - BACKEND_URL=http://backend:3001
      - FRONTEND_URL=http://frontend:3000
      - PORT=3002
    depends_on:
      ollama-milhena:
        condition: service_healthy
      ollama-setup:
        condition: service_completed_successfully
    volumes:
      - ../ai-agent/logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  ollama_milhena_models:
    name: pilotpros_ollama_milhena_models